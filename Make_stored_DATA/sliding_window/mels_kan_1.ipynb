{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2912ce2-8983-4c2e-ac83-db53af6b6bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_type = 'mels_kan_1'\n",
    "sample_rate = 2000\n",
    "n_fft, n_hop_length = 512, 512\n",
    "window_size, hop_length = int(sample_rate*1.0), sample_rate/4\n",
    "n_mels = 256\n",
    "seed = 958"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "543ffe40-6504-4351-822d-c3879b17ff55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import librosa\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41c7d58e-0a0d-4f71-8165-e142eeb1af0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2620,) (620,)\n"
     ]
    }
   ],
   "source": [
    "heart_sound_all = glob.glob('../../heart-new/training/*.wav')\n",
    "heart_sound, heart_sound_test = heart_sound_all[:2620], heart_sound_all[2620:]\n",
    "print(np.shape(heart_sound),np.shape(heart_sound_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "261db4d2-891c-4474-af80-291deb1d7739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2620,) (620,)\n"
     ]
    }
   ],
   "source": [
    "with open('../../heart-new/label-all/RECORDS') as file:Record=file.read().splitlines()\n",
    "\n",
    "def process_labels(folder):\n",
    "    with open(f'../../heart-new/{folder}/RECORDS') as file:\n",
    "        records = file.read().splitlines()\n",
    "    with open(f'../../heart-new/{folder}/RECORDS-normal') as file:\n",
    "        normal = set(file.read().splitlines())\n",
    "    with open(f'../../heart-new/{folder}/RECORDS-abnormal') as file:\n",
    "        abnormal = set(file.read().splitlines())\n",
    "    normal_dict = {item: 0 for item in list(normal)}\n",
    "    abnormal_dict = {item: 1 for item in list(abnormal)}\n",
    "    combined_dict = {**normal_dict, **abnormal_dict}\n",
    "    reordered_dict = {k: combined_dict[k] for k in records}\n",
    "    label_list = list(reordered_dict.values())\n",
    "    return label_list\n",
    "\n",
    "LABEL_list_all = process_labels('label-all')\n",
    "LABEL_list, LABEL_list_test = LABEL_list_all[:2620], LABEL_list_all[2620:]\n",
    "LABEL_array, LABEL_array_test = np.array(LABEL_list), np.array(LABEL_list_test)\n",
    "print(np.shape(LABEL_array), np.shape(LABEL_array_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e24a5d69-1fad-4e66-a077-8f23e24b6edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(heart_sounds, sr=sample_rate):\n",
    "    data = []\n",
    "    for i in tqdm(range(len(heart_sounds)),desc='Import data',leave=False):\n",
    "        y, sr = librosa.load(heart_sounds[i], sr=sr)\n",
    "        if len(y) >= sample_rate*5 : data.append(y[:sample_rate*5])\n",
    "        else:data.append(y)\n",
    "    return data\n",
    "    \n",
    "def augment_data(signals, labels, sr=sample_rate, fraction=1):\n",
    "    num_to_augment = int(len(signals) * fraction)\n",
    "    indices_to_augment = random.Random(seed).sample(range(len(signals)), num_to_augment)\n",
    "    aug_signals,aug_labels = [],[]\n",
    "    methods = ['pitch_shift', 'time_stretch', 'add_noise']\n",
    "    for idx in tqdm(indices_to_augment,desc='Augmenting',total=num_to_augment,leave=False):\n",
    "        signal = signals[idx]\n",
    "        label = labels[idx]\n",
    "        method = random.Random(seed).choice(methods)\n",
    "        if method == 'pitch_shift':\n",
    "            n_semitones = random.Random(seed).uniform(-2, 2)\n",
    "            augmented_signal = librosa.effects.pitch_shift(y=signal, sr=sample_rate, n_steps=n_semitones)\n",
    "        elif method == 'time_stretch':\n",
    "            rate = random.Random(seed).uniform(0.8, 1.2)\n",
    "            augmented_signal = librosa.effects.time_stretch(signal, rate=rate)\n",
    "        elif method == 'add_noise':\n",
    "            noise_factor = random.Random(seed).uniform(0.001, 0.01)\n",
    "            noise = noise_factor * np.random.Random(seed).randn(len(signal))\n",
    "            augmented_signal = signal + noise\n",
    "        aug_signals.append(augmented_signal)\n",
    "        aug_labels.append(label)\n",
    "    return signals, list(labels), aug_signals, list(aug_labels)\n",
    "\n",
    "def sliding_window(signals, labels, window_size=window_size, hop_length=hop_length):\n",
    "    segments = []\n",
    "    segment_labels = []\n",
    "    for signal, label in tqdm(zip(signals, labels),desc='Segmenting',total=len(labels),leave=False):\n",
    "        if len(signal) < window_size:\n",
    "            continue\n",
    "        num_segments = int((len(signal)-hop_length)/(window_size-hop_length))\n",
    "        for i in range(num_segments):\n",
    "            start = int(i * (window_size-hop_length))\n",
    "            end = start + window_size\n",
    "            window = signal[start:end]\n",
    "            segments.append(window)\n",
    "            segment_labels.append(label)\n",
    "    return segments, segment_labels\n",
    "\n",
    "def double_shuffle(signals, labels):\n",
    "    Z = list(zip(signals, labels))\n",
    "    random.Random(seed).shuffle(Z)\n",
    "    shuffled_data, shuffled_label = zip(*Z)\n",
    "    shuffled_data, shuffled_label = np.array(shuffled_data), np.array(shuffled_label)\n",
    "    return shuffled_data, shuffled_label\n",
    "\n",
    "def balance_classes(signals, labels):\n",
    "    signals = np.array(signals, dtype=np.float16)\n",
    "    labels = np.array(labels, dtype=np.int16)\n",
    "    undersampler = RandomUnderSampler(random_state=seed)\n",
    "    signals_resampled, labels_resampled = undersampler.fit_resample(signals, labels)\n",
    "    return signals_resampled, labels_resampled\n",
    "\n",
    "def extract_features(signals):\n",
    "    mel_spectrograms = []\n",
    "    scaler = MinMaxScaler()\n",
    "    for signal in tqdm(signals,desc='Extracting features',total=len(signals),leave=False):\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=signal, sr=sample_rate, n_fft=n_fft, hop_length=n_hop_length, n_mels=n_mels)\n",
    "        mel_spectrogram = librosa.amplitude_to_db(mel_spectrogram, ref=np.max)\n",
    "        mel_spectrogram_flat = mel_spectrogram.flatten().reshape(-1,1)\n",
    "        mel_spectrogram_normalized = scaler.fit_transform(mel_spectrogram_flat)\n",
    "        mel_spectrogram_normalized = mel_spectrogram_normalized.reshape(mel_spectrogram.shape)\n",
    "        mel_spectrograms.append(mel_spectrogram_normalized)\n",
    "    return np.array(mel_spectrograms, dtype=np.float32)\n",
    "\n",
    "def preprocessing(signals, labels, length=-1):\n",
    "    signals_windowed, labels_windowed = sliding_window(signals, labels)\n",
    "    signals_balanced, labels_balanced = balance_classes(signals_windowed, labels_windowed)\n",
    "    signals_shuffled, labels_shuffled = double_shuffle(signals_balanced, labels_balanced)\n",
    "    signals_reshaped, labels_reshaped = signals_shuffled[:length], labels_shuffled[:length]\n",
    "    signal = extract_features(signals_reshaped)\n",
    "    label = list(labels_reshaped)\n",
    "    return signal, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6b3b44b-0067-4513-b487-8ac1959a1220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing train & validation dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing and fold assignment complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing train & validation dataset...\")\n",
    "DATA_array = load_data(heart_sound)\n",
    "Training_data, Training_label, augmented_data, augmented_label = augment_data(DATA_array, LABEL_array)\n",
    "Training_data_preprocessed, Training_label_preprocessed = preprocessing(Training_data, Training_label)\n",
    "augmented_data_preprocessed, augmented_label_preprocessed = preprocessing(augmented_data, augmented_label, length=9000-np.shape(Training_label_preprocessed)[0])\n",
    "DATA = np.concatenate([Training_data_preprocessed, augmented_data_preprocessed], axis=0)\n",
    "LABEL = np.array(Training_label_preprocessed + augmented_label_preprocessed, dtype=np.int32)\n",
    "\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "folds = list(kf.split(DATA))\n",
    "\n",
    "EVERY_Fold_data_train = [[] for i in range(k)]\n",
    "EVERY_Fold_data_val = [[] for i in range(k)]\n",
    "EVERY_Fold_label_train = [[] for i in range(k)]\n",
    "EVERY_Fold_label_val = [[] for i in range(k)]\n",
    "\n",
    "for i, (train_idx, val_idx) in enumerate(folds):\n",
    "    EVERY_Fold_data_train[i].append(DATA[train_idx])\n",
    "    EVERY_Fold_label_train[i].append(LABEL[train_idx])\n",
    "    EVERY_Fold_data_val[i].append(DATA[val_idx])\n",
    "    EVERY_Fold_label_val[i].append(LABEL[val_idx])\n",
    "\n",
    "print(\"Preprocessing and fold assignment complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd9e8c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing test dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing test dataset...\")\n",
    "DATA_array_test = load_data(heart_sound_test)\n",
    "Test_data, Test_label = preprocessing(DATA_array_test, LABEL_array_test, length=1000)\n",
    "print(\"Preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cfe7517-c9ca-41ff-94da-ccddfc60f1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 7200, 256, 4)\n",
      "(5, 7200)\n",
      "(5, 1800, 256, 4)\n",
      "(5, 1800)\n",
      "(1000, 256, 4)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "EVERY_Fold_data_train_download=[[],[],[],[],[]]\n",
    "EVERY_Fold_label_train_download=[[],[],[],[],[]]\n",
    "EVERY_Fold_data_val_download=[[],[],[],[],[]]\n",
    "EVERY_Fold_label_val_download=[[],[],[],[],[]]\n",
    "for i in range(5):EVERY_Fold_data_train_download[i] = EVERY_Fold_data_train[i][0]\n",
    "for i in range(5):EVERY_Fold_label_train_download[i] = EVERY_Fold_label_train[i][0]\n",
    "for i in range(5):EVERY_Fold_data_val_download[i] = EVERY_Fold_data_val[i][0]\n",
    "for i in range(5):EVERY_Fold_label_val_download[i] = EVERY_Fold_label_val[i][0]\n",
    "print(np.shape(EVERY_Fold_data_train_download))\n",
    "print(np.shape(EVERY_Fold_label_train_download))\n",
    "print(np.shape(EVERY_Fold_data_val_download))\n",
    "print(np.shape(EVERY_Fold_label_val_download))\n",
    "print(np.shape(Test_data))\n",
    "print(np.shape(Test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "237ecd5a-b3d4-4cbe-9721-c3d08977d661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data have lebel normal : 49.58% and have lebel abnormal : 50.42%\n",
      "Validation data have lebel normal : 49.94% and have lebel abnormal : 50.06%\n",
      "Test data have lebel normal : 50.20% and have lebel abnormal : 49.80%\n"
     ]
    }
   ],
   "source": [
    "train_fraction , val_fraction = len(LABEL)*(k-1)/(100*k) , len(LABEL)/(100*k)\n",
    "print(f'Train data have lebel normal : {(list(EVERY_Fold_label_train_download[0]).count(0))/train_fraction:.2f}% and have lebel abnormal : {(list(EVERY_Fold_label_train_download[0]).count(1))/train_fraction:.2f}%')\n",
    "print(f'Validation data have lebel normal : {(list(EVERY_Fold_label_val_download[0]).count(0))/val_fraction:.2f}% and have lebel abnormal : {(list(EVERY_Fold_label_val_download[0]).count(1))/val_fraction:.2f}%')\n",
    "print(f'Test data have lebel normal : {100*(list(Test_label).count(0))/len(Test_label):.2f}% and have lebel abnormal : {100*(list(Test_label).count(1))/len(Test_label):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b170e14-d478-4827-b68c-c7148342e482",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'{DATA_type}/Train_data.npy', EVERY_Fold_data_train_download)\n",
    "np.save(f'{DATA_type}/Train_label.npy', EVERY_Fold_label_train_download)\n",
    "np.save(f'{DATA_type}/Val_data.npy', EVERY_Fold_data_val_download)\n",
    "np.save(f'{DATA_type}/Val_label.npy', EVERY_Fold_label_val_download)\n",
    "np.save(f'{DATA_type}/Test_data.npy', Test_data)\n",
    "np.save(f'{DATA_type}/Test_label.npy', Test_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflowgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
