{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2912ce2-8983-4c2e-ac83-db53af6b6bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_type = 'MELS_rnn'\n",
    "sample_rate = 2000\n",
    "n_fft, n_hop_length = 512, 512\n",
    "window_size, hop_length = int(sample_rate*1.0), sample_rate/4\n",
    "n_mels = 256\n",
    "seed = 958"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "543ffe40-6504-4351-822d-c3879b17ff55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import glob\n",
    "import random\n",
    "import librosa\n",
    "import IPython.display\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib as plt\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.signal import butter, lfilter\n",
    "from tensorflow.keras import models, layers, Input\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPooling2D, Dropout, Reshape, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41c7d58e-0a0d-4f71-8165-e142eeb1af0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3240,)\n"
     ]
    }
   ],
   "source": [
    "heart_sound = glob.glob('../heart-new/training/*.wav')\n",
    "print(np.shape(heart_sound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c418dff6-26a5-4c10-b2bc-966a907a9a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3240,)\n"
     ]
    }
   ],
   "source": [
    "with open('../heart-new/lebel-all/RECORDS') as file:Record=file.read().splitlines()\n",
    "print(np.shape(Record))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "261db4d2-891c-4474-af80-291deb1d7739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3240,)\n"
     ]
    }
   ],
   "source": [
    "def process_labels(folder):\n",
    "    with open(f'../heart-new/{folder}/RECORDS') as file:\n",
    "        records = file.read().splitlines()\n",
    "    with open(f'../heart-new/{folder}/RECORDS-normal') as file:\n",
    "        normal = set(file.read().splitlines())\n",
    "    with open(f'../heart-new/{folder}/RECORDS-abnormal') as file:\n",
    "        abnormal = set(file.read().splitlines())\n",
    "    normal_dict = {item: 0 for item in list(normal)}\n",
    "    abnormal_dict = {item: 1 for item in list(abnormal)}\n",
    "    combined_dict = {**normal_dict, **abnormal_dict}\n",
    "    reordered_dict = {k: combined_dict[k] for k in records}\n",
    "    label_list = list(reordered_dict.values())\n",
    "    return label_list\n",
    "\n",
    "LEBEL_list = process_labels('lebel-all')\n",
    "LEBEL_array = np.array(LEBEL_list)\n",
    "print(np.shape(LEBEL_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e24a5d69-1fad-4e66-a077-8f23e24b6edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(heart_sounds, sr=sample_rate):\n",
    "    data = []\n",
    "    for i in tqdm(range(len(heart_sounds)),desc='Import data',leave=False):\n",
    "        y, sr = librosa.load(heart_sounds[i], sr=sr)\n",
    "        if len(y) >= sample_rate*5 : data.append(y[:sample_rate*5])\n",
    "        else:data.append(y)\n",
    "    return data\n",
    "    \n",
    "def augment_data(signals, labels, sr=sample_rate, fraction=1):\n",
    "    num_to_augment = int(len(signals) * fraction)\n",
    "    indices_to_augment = random.Random(seed).sample(range(len(signals)), num_to_augment)\n",
    "    aug_signals,aug_labels = [],[]\n",
    "    methods = ['pitch_shift', 'time_stretch', 'add_noise']\n",
    "    for idx in tqdm(indices_to_augment,desc='Augmenting',total=num_to_augment,leave=False):\n",
    "        signal = signals[idx]\n",
    "        label = labels[idx]\n",
    "        method = random.Random(seed).choice(methods)\n",
    "        if method == 'pitch_shift':\n",
    "            n_semitones = random.Random(seed).uniform(-2, 2)\n",
    "            augmented_signal = librosa.effects.pitch_shift(y=signal, sr=sample_rate, n_steps=n_semitones)\n",
    "        elif method == 'time_stretch':\n",
    "            rate = random.Random(seed).uniform(0.8, 1.2)\n",
    "            augmented_signal = librosa.effects.time_stretch(signal, rate=rate)\n",
    "        elif method == 'add_noise':\n",
    "            noise_factor = random.Random(seed).uniform(0.001, 0.01)\n",
    "            noise = noise_factor * np.random.Random(seed).randn(len(signal))\n",
    "            augmented_signal = signal + noise\n",
    "        aug_signals.append(augmented_signal)\n",
    "        aug_labels.append(label)\n",
    "    return signals, list(labels), aug_signals, list(aug_labels)\n",
    "\n",
    "def sliding_window(signals, labels, window_size=window_size, hop_length=hop_length):\n",
    "    segments = []\n",
    "    segment_labels = []\n",
    "    for signal, label in tqdm(zip(signals, labels),desc='Segmenting',total=len(labels),leave=False):\n",
    "        if len(signal) < window_size:\n",
    "            continue\n",
    "        num_segments = int((len(signal)-hop_length)/(window_size-hop_length))\n",
    "        for i in range(num_segments):\n",
    "            start = int(i * (window_size-hop_length))\n",
    "            end = start + window_size\n",
    "            window = signal[start:end]\n",
    "            segments.append(window)\n",
    "            segment_labels.append(label)\n",
    "    return segments, segment_labels\n",
    "\n",
    "def double_shuffle(signals, lebels):\n",
    "    Z = list(zip(signals, lebels))\n",
    "    random.Random(seed).shuffle(Z)\n",
    "    shuffled_data, shuffled_lebel = zip(*Z)\n",
    "    shuffled_data, shuffled_lebel = np.array(shuffled_data), np.array(shuffled_lebel)\n",
    "    return shuffled_data, shuffled_lebel\n",
    "\n",
    "def balance_classes(signals, labels):\n",
    "    signals = np.array(signals, dtype=np.float16)\n",
    "    labels = np.array(labels, dtype=np.int16)\n",
    "    undersampler = RandomUnderSampler(random_state=seed)\n",
    "    signals_resampled, labels_resampled = undersampler.fit_resample(signals, labels)\n",
    "    return signals_resampled, labels_resampled\n",
    "\n",
    "def extract_features(signals):\n",
    "    mel_spectrograms = []\n",
    "    scaler = MinMaxScaler()\n",
    "    for signal in tqdm(signals,desc='Extracting features',total=len(signals),leave=False):\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=signal, sr=sample_rate, n_fft=n_fft, hop_length=n_hop_length, n_mels=n_mels)\n",
    "        mel_spectrogram = librosa.amplitude_to_db(mel_spectrogram, ref=np.max)\n",
    "        mel_spectrogram_flat = mel_spectrogram.flatten().reshape(-1,1)\n",
    "        mel_spectrogram_normalized = scaler.fit_transform(mel_spectrogram_flat)\n",
    "        mel_spectrogram_normalized = mel_spectrogram_normalized.reshape(mel_spectrogram.shape)\n",
    "        mel_spectrogram_reshaped   = mel_spectrogram_normalized.T\n",
    "        mel_spectrograms.append(mel_spectrogram_reshaped)\n",
    "    return np.array(mel_spectrograms, dtype=np.float32)\n",
    "\n",
    "def preprocessing(signals, lebels, length=-1):\n",
    "    signals_windowed, lebels_windowed = sliding_window(signals, lebels)\n",
    "    signals_balanced, lebels_balanced = balance_classes(signals_windowed, lebels_windowed)\n",
    "    signals_shuffled, lebels_shuffled = double_shuffle(signals_balanced, lebels_balanced)\n",
    "    signals_reshaped, lebels_reshaped = signals_shuffled[:length], lebels_shuffled[:length]\n",
    "    signal = extract_features(signals_reshaped)\n",
    "    lebel = list(lebels_reshaped)\n",
    "    return signal, lebel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6b3b44b-0067-4513-b487-8ac1959a1220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing entire dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing and fold assignment complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing entire dataset...\")\n",
    "DATA_array = load_data(heart_sound)\n",
    "Training_data, Training_lebel, augmented_data, augmented_lebel = augment_data(DATA_array, LEBEL_array)\n",
    "Training_data_preprocessed, Training_lebel_preprocessed = preprocessing(Training_data, Training_lebel)\n",
    "augmented_data_preprocessed, augmented_lebel_preprocessed = preprocessing(augmented_data, augmented_lebel, length=10000-np.shape(Training_lebel_preprocessed)[0])\n",
    "DATA = np.concatenate([Training_data_preprocessed, augmented_data_preprocessed], axis=0)\n",
    "LEBEL = np.array(Training_lebel_preprocessed + augmented_lebel_preprocessed, dtype=np.int32)\n",
    "\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "folds = list(kf.split(DATA))\n",
    "\n",
    "EVERY_Fold_data_train = [[] for i in range(k)]\n",
    "EVERY_Fold_data_val = [[] for i in range(k)]\n",
    "EVERY_Fold_lebel_train = [[] for i in range(k)]\n",
    "EVERY_Fold_lebel_val = [[] for i in range(k)]\n",
    "\n",
    "for i, (train_idx, val_idx) in enumerate(folds):\n",
    "    EVERY_Fold_data_train[i].append(DATA[train_idx])\n",
    "    EVERY_Fold_lebel_train[i].append(LEBEL[train_idx])\n",
    "    EVERY_Fold_data_val[i].append(DATA[val_idx])\n",
    "    EVERY_Fold_lebel_val[i].append(LEBEL[val_idx])\n",
    "\n",
    "print(\"Preprocessing and fold assignment complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cfe7517-c9ca-41ff-94da-ccddfc60f1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 8000, 4, 256)\n",
      "(5, 8000)\n",
      "(5, 2000, 4, 256)\n",
      "(5, 2000)\n"
     ]
    }
   ],
   "source": [
    "EVERY_Fold_data_train_download=[[],[],[],[],[]]\n",
    "EVERY_Fold_lebel_train_download=[[],[],[],[],[]]\n",
    "EVERY_Fold_data_val_download=[[],[],[],[],[]]\n",
    "EVERY_Fold_lebel_val_download=[[],[],[],[],[]]\n",
    "for i in range(5):EVERY_Fold_data_train_download[i] = EVERY_Fold_data_train[i][0]\n",
    "for i in range(5):EVERY_Fold_lebel_train_download[i] = EVERY_Fold_lebel_train[i][0]\n",
    "for i in range(5):EVERY_Fold_data_val_download[i] = EVERY_Fold_data_val[i][0]\n",
    "for i in range(5):EVERY_Fold_lebel_val_download[i] = EVERY_Fold_lebel_val[i][0]\n",
    "print(np.shape(EVERY_Fold_data_train_download))\n",
    "print(np.shape(EVERY_Fold_lebel_train_download))\n",
    "print(np.shape(EVERY_Fold_data_val_download))\n",
    "print(np.shape(EVERY_Fold_lebel_val_download))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "237ecd5a-b3d4-4cbe-9721-c3d08977d661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data have lebel normal : 49.91% and have lebel abnormal : 50.09%\n",
      "Validation data have lebel normal : 51.40% and have lebel abnormal : 48.60%\n"
     ]
    }
   ],
   "source": [
    "train_fraction , test_fraction = len(LEBEL)*(k-1)/(100*k) , len(LEBEL)/(100*k)\n",
    "print(f'Train data have lebel normal : {(list(EVERY_Fold_lebel_train_download[0]).count(0))/train_fraction:.2f}% and have lebel abnormal : {(list(EVERY_Fold_lebel_train_download[0]).count(1))/train_fraction:.2f}%')\n",
    "print(f'Validation data have lebel normal : {(list(EVERY_Fold_lebel_val_download[0]).count(0))/test_fraction:.2f}% and have lebel abnormal : {(list(EVERY_Fold_lebel_val_download[0]).count(1))/test_fraction:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b170e14-d478-4827-b68c-c7148342e482",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'{DATA_type}/Train_data.npy', EVERY_Fold_data_train_download)\n",
    "np.save(f'{DATA_type}/Train_lebel.npy', EVERY_Fold_lebel_train_download)\n",
    "np.save(f'{DATA_type}/Val_data.npy', EVERY_Fold_data_val_download)\n",
    "np.save(f'{DATA_type}/Val_lebel.npy', EVERY_Fold_lebel_val_download)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
